#!/usr/bin/env python3
"""
íŠ¹ì • ë‚ ì§œ ë‰´ìŠ¤ í´ë¦¬í•‘ ìŠ¤í¬ë¦½íŠ¸

ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ì¤‘ íŠ¹ì • ë‚ ì§œì— ë°œí–‰ëœ ë‰´ìŠ¤ë§Œ í•„í„°ë§í•˜ì—¬ í´ë¦¬í•‘í•©ë‹ˆë‹¤.
"""

import sys
import argparse
from pathlib import Path
from datetime import datetime, date, timedelta

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

# .env íŒŒì¼ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv(project_root / ".env")

from loguru import logger
from utils.config import get_settings
from utils.logger import setup_logger
from collector import NewsCollector
from analyzer import NewsAnalyzer
from publisher import NotionPublisher
from storage import NewsDatabase
from analyzer.gemini_client import GeminiAnalyzer


def run_clipper_for_date(target_date: date, period: str, settings, publisher, collector, analyzer, database, gemini):
    """íŠ¹ì • ë‚ ì§œì— ëŒ€í•´ ë‰´ìŠ¤ í´ë¦¬í•‘ ì‹¤í–‰"""
    
    logger.info(f"\n{'='*60}")
    logger.info(f"ğŸ“… {target_date} {period} ë‰´ìŠ¤ í´ë¦¬í•‘ ì‹œì‘")
    logger.info(f"{'='*60}")
    
    # í‚¤ì›Œë“œ ì¡°í•© ë¡œë“œ
    keyword_combinations = settings.get_keyword_combinations()
    config = settings.load_config()
    
    try:
        # 1. ë‰´ìŠ¤ ìˆ˜ì§‘ (ìµœëŒ€ ë²”ìœ„ë¡œ ìˆ˜ì§‘)
        logger.info(f"ğŸ“° Step 1: ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘... (ìµœê·¼ 7ì¼)")
        
        articles = collector.collect_all(
            keyword_combinations=keyword_combinations,
            max_results_per_combo=50,  # ë” ë§ì´ ìˆ˜ì§‘
            use_naver=bool(settings.naver_client_id),
            when="7d"  # ìµœê·¼ 7ì¼
        )
        
        if not articles:
            logger.warning("ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
            return None
        
        logger.info(f"ğŸ“¥ ìˆ˜ì§‘ ì™„ë£Œ: {len(articles)}ê±´")
        
        # 1.5. ì–¸ë¡ ì‚¬ í•„í„°ë§
        logger.info("ğŸ“° Step 1.5: ì–¸ë¡ ì‚¬ í•„í„°ë§ ì¤‘...")
        news_sources = config.get("news_sources", {})
        allowed_domains = []
        
        for media in news_sources.get("priority_media", []):
            allowed_domains.append(media.get("domain", ""))
        for media in news_sources.get("national_media", []):
            allowed_domains.append(media.get("domain", ""))
        
        if allowed_domains:
            original_count = len(articles)
            articles = [
                article for article in articles
                if any(domain in (article.url or "") for domain in allowed_domains)
            ]
            logger.info(f"ğŸ¢ ì–¸ë¡ ì‚¬ í•„í„°ë§: {original_count}ê±´ â†’ {len(articles)}ê±´")
        
        # 2. ë‚ ì§œ í•„í„°ë§ (í•µì‹¬!)
        logger.info(f"ğŸ“… Step 2: {target_date} ë‚ ì§œ í•„í„°ë§ ì¤‘...")
        
        # ì˜¤ì „/ì˜¤í›„ì— ë”°ë¥¸ ì‹œê°„ ë²”ìœ„ ì„¤ì •
        if period == "ì˜¤ì „":
            # ì „ë‚  18ì‹œ ~ ë‹¹ì¼ 10ì‹œ
            start_time = datetime.combine(target_date - timedelta(days=1), datetime.strptime("18:00", "%H:%M").time())
            end_time = datetime.combine(target_date, datetime.strptime("10:00", "%H:%M").time())
        else:  # ì˜¤í›„
            # ë‹¹ì¼ 10ì‹œ ~ ë‹¹ì¼ 18ì‹œ
            start_time = datetime.combine(target_date, datetime.strptime("10:00", "%H:%M").time())
            end_time = datetime.combine(target_date, datetime.strptime("18:00", "%H:%M").time())
        
        date_filtered = []
        for article in articles:
            if article.published_at:
                # published_atì´ datetimeì¸ ê²½ìš°
                if isinstance(article.published_at, datetime):
                    pub_date = article.published_at.date()
                else:
                    pub_date = article.published_at
                
                # í•´ë‹¹ ë‚ ì§œì˜ ë‰´ìŠ¤ë§Œ í¬í•¨
                if pub_date == target_date:
                    date_filtered.append(article)
            else:
                # ë°œí–‰ì¼ì´ ì—†ìœ¼ë©´ ì œëª©/ì„¤ëª…ì—ì„œ ë‚ ì§œ ì¶”ë¡  ì‹œë„ (ìŠ¤í‚µ)
                pass
        
        if not date_filtered:
            # ë‚ ì§œ í•„í„°ë§ì´ ì•ˆ ë˜ë©´ ì „ì²´ ì¤‘ ì¼ë¶€ë§Œ ì‚¬ìš©
            logger.warning(f"âš ï¸ {target_date} ë‚ ì§œì˜ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìµœê·¼ ë‰´ìŠ¤ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            date_filtered = articles[:30]  # ìµœê·¼ 30ê°œë§Œ
        
        articles = date_filtered
        logger.info(f"ğŸ“… ë‚ ì§œ í•„í„°ë§ ì™„ë£Œ: {len(articles)}ê±´")
        
        if not articles:
            logger.warning("í•„í„°ë§ í›„ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
            return None
        
        # 3. AI ë¶„ì„ ë° í•„í„°ë§
        logger.info("ğŸ¤– Step 3: AI ë¶„ì„ ì¤‘...")
        passed_articles, filtered_articles = analyzer.analyze_and_filter(
            articles=articles,
            summarize=True
        )
        
        if not passed_articles:
            logger.info("ê´€ë ¨ì„± ë†’ì€ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
            return None
        
        # ì¤‘ìš”ë„ìˆœ ì •ë ¬
        passed_articles = analyzer.sort_by_importance(passed_articles)
        logger.info(f"âœ… ë¶„ì„ ì™„ë£Œ: ê´€ë ¨ ë‰´ìŠ¤ {len(passed_articles)}ê±´")
        
        # 3.5. ì¸ì‚¬ì´íŠ¸ ìƒì„±
        logger.info("ğŸ’¡ Step 3.5: ì¸ì‚¬ì´íŠ¸ ìƒì„± ì¤‘...")
        insight = gemini.generate_daily_insight(passed_articles)
        logger.info(f"ğŸ’¡ ì¸ì‚¬ì´íŠ¸ ìƒì„± ì™„ë£Œ: {insight.get('headline', '')[:50]}...")
        
        # 4. ë…¸ì…˜ ë°œí–‰
        logger.info("ğŸ“¤ Step 4: ë…¸ì…˜ ë°œí–‰ ì¤‘...")
        results = publisher.publish_articles(
            articles=passed_articles,
            create_summary=True,
            insight=insight,
            period=period,
            target_date=target_date
        )
        
        logger.info(f"ğŸ“ ë°œí–‰ ì™„ë£Œ: ì„±ê³µ {len(results['success'])}ê±´")
        
        # 5. DB ì €ì¥
        logger.info("ğŸ’¾ Step 5: ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘...")
        database.save_articles(passed_articles)
        
        return results
        
    except Exception as e:
        logger.exception(f"âŒ {target_date} {period} í´ë¦¬í•‘ ì¤‘ ì˜¤ë¥˜: {e}")
        return None


def main():
    parser = argparse.ArgumentParser(description="íŠ¹ì • ë‚ ì§œ ë‰´ìŠ¤ í´ë¦¬í•‘")
    parser.add_argument("--date", "-d", type=str, required=True, help="í´ë¦¬í•‘í•  ë‚ ì§œ (YYYY-MM-DD)")
    parser.add_argument("--period", "-p", type=str, choices=["ì˜¤ì „", "ì˜¤í›„", "both"], default="both", help="ê¸°ê°„")
    args = parser.parse_args()
    
    # ë‚ ì§œ íŒŒì‹±
    try:
        target_date = datetime.strptime(args.date, "%Y-%m-%d").date()
    except ValueError:
        print("âŒ ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. YYYY-MM-DD í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        sys.exit(1)
    
    # ì„¤ì • ë¡œë“œ
    settings = get_settings()
    
    # ë¡œê±° ì„¤ì •
    setup_logger(
        log_level=settings.log_level,
        log_file=Path("logs") / f"clip_{target_date}_{datetime.now().strftime('%H%M%S')}.log"
    )
    
    logger.info("=" * 60)
    logger.info(f"ğŸš€ {target_date} ë‰´ìŠ¤ í´ë¦¬í•‘ ì‹œì‘")
    logger.info("=" * 60)
    
    # ì„¤ì • ê²€ì¦
    if not settings.notion_database_id:
        logger.error("âŒ NOTION_DATABASE_IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        sys.exit(1)
    
    # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
    collector = NewsCollector(
        naver_client_id=settings.naver_client_id,
        naver_client_secret=settings.naver_client_secret
    )
    
    analyzer = NewsAnalyzer(
        api_key=settings.google_api_key,
        relevance_threshold=settings.relevance_threshold
    )
    
    publisher = NotionPublisher(
        api_key=settings.notion_api_key,
        database_id=settings.notion_database_id
    )
    
    database = NewsDatabase(db_path=settings.db_path)
    
    gemini = GeminiAnalyzer(
        api_key=settings.google_api_key,
        is_paid_plan=True
    )
    
    # í´ë¦¬í•‘ ì‹¤í–‰
    if args.period == "both":
        # ì˜¤ì „
        run_clipper_for_date(
            target_date=target_date,
            period="ì˜¤ì „",
            settings=settings,
            publisher=publisher,
            collector=collector,
            analyzer=analyzer,
            database=database,
            gemini=gemini
        )
        
        # ì˜¤í›„
        run_clipper_for_date(
            target_date=target_date,
            period="ì˜¤í›„",
            settings=settings,
            publisher=publisher,
            collector=collector,
            analyzer=analyzer,
            database=database,
            gemini=gemini
        )
    else:
        run_clipper_for_date(
            target_date=target_date,
            period=args.period,
            settings=settings,
            publisher=publisher,
            collector=collector,
            analyzer=analyzer,
            database=database,
            gemini=gemini
        )
    
    logger.info("\n" + "=" * 60)
    logger.info(f"âœ… {target_date} ë‰´ìŠ¤ í´ë¦¬í•‘ ì™„ë£Œ!")
    logger.info("=" * 60)


if __name__ == "__main__":
    main()

