"""ë‰´ìŠ¤ ë¶„ì„ í†µí•© ëª¨ë“ˆ"""

import re
import sys
from pathlib import Path
from typing import List, Tuple, Dict
from difflib import SequenceMatcher

from loguru import logger

# ìƒìœ„ ë””ë ‰í† ë¦¬ importë¥¼ ìœ„í•œ ê²½ë¡œ ì„¤ì •
src_dir = Path(__file__).parent.parent
if str(src_dir) not in sys.path:
    sys.path.insert(0, str(src_dir))

from collector.models import NewsArticle
from analyzer.gemini_client import GeminiAnalyzer


class NewsAnalyzer:
    """ë‰´ìŠ¤ ë¶„ì„ ë° í•„í„°ë§ í†µí•© í´ë˜ìŠ¤"""
    
    # ê²½ë‚¨ ì§€ì—­ ìš°ì„  ì–¸ë¡ ì‚¬ (ê°€ì‚°ì  ë¶€ì—¬)
    PRIORITY_MEDIA_DOMAINS = {
        'idomin.com': 2,        # ê²½ë‚¨ë„ë¯¼ì¼ë³´: +2ì 
        'knnews.co.kr': 2,      # ê²½ë‚¨ì‹ ë¬¸: +2ì 
        'changwon.kbs.co.kr': 2, # KBSì°½ì›: +2ì 
        'mbcgn.kr': 2,          # MBCê²½ë‚¨: +2ì 
    }
    
    def __init__(
        self,
        api_key: str,
        relevance_threshold: int = 60,
        model: str = "gemini-2.5-flash-lite",
        is_paid_plan: bool = True
    ):
        """
        Args:
            api_key: Google AI API í‚¤
            relevance_threshold: ê´€ë ¨ì„± ì„ê³„ê°’ (ì´ ê°’ ì´ìƒë§Œ í†µê³¼)
            model: ì‚¬ìš©í•  Gemini ëª¨ë¸ (gemini-1.5-pro ê¶Œì¥)
            is_paid_plan: ìœ ë£Œ í”Œëœ ì—¬ë¶€
        """
        self.gemini = GeminiAnalyzer(
            api_key=api_key, 
            model=model,
            is_paid_plan=is_paid_plan
        )
        self.relevance_threshold = relevance_threshold
        self.is_paid_plan = is_paid_plan
    
    def _get_media_bonus(self, url: str) -> int:
        """ì–¸ë¡ ì‚¬ ê°€ì‚°ì  ê³„ì‚°"""
        for domain, bonus in self.PRIORITY_MEDIA_DOMAINS.items():
            if domain in url:
                return bonus
        return 0
    
    def _is_priority_media(self, url: str) -> bool:
        """ìš°ì„  ì–¸ë¡ ì‚¬ ì—¬ë¶€ í™•ì¸"""
        return any(domain in url for domain in self.PRIORITY_MEDIA_DOMAINS.keys())
    
    def analyze_and_filter(
        self,
        articles: List[NewsArticle],
        summarize: bool = True,
        use_batch: bool = True,
        batch_size: int = 5
    ) -> Tuple[List[NewsArticle], List[NewsArticle]]:
        """ë‰´ìŠ¤ ë¶„ì„ ë° í•„í„°ë§
        
        Args:
            articles: ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
            summarize: ìš”ì•½ ìƒì„± ì—¬ë¶€
            use_batch: ë°°ì¹˜ ë¶„ì„ ì‚¬ìš© ì—¬ë¶€ (ìœ ë£Œ í”Œëœ ê¶Œì¥)
            batch_size: ë°°ì¹˜ í¬ê¸°
            
        Returns:
            (í†µê³¼ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸, ì œì™¸ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸)
        """
        passed_articles = []
        filtered_articles = []
        
        logger.info(f"=== ë‰´ìŠ¤ ë¶„ì„ ì‹œì‘: {len(articles)}ê±´ ===")
        
        # ë°°ì¹˜ ë¶„ì„ (ìœ ë£Œ í”Œëœì—ì„œ íš¨ìœ¨ì )
        if use_batch and self.is_paid_plan:
            logger.info(f"ë°°ì¹˜ ë¶„ì„ ëª¨ë“œ (ë°°ì¹˜ í¬ê¸°: {batch_size})")
            filter_results = self.gemini.batch_analyze(articles, batch_size=batch_size)
            
            for i, (article, result) in enumerate(zip(articles, filter_results)):
                if isinstance(result, dict):
                    article.relevance_score = result.get("relevance_score", 0)
                    article.importance_score = result.get("importance_score", 1)
                    if result.get("category"):
                        article.category = result["category"]
                    
                    is_relevant = result.get("is_relevant", False)
                    meets_threshold = article.relevance_score >= self.relevance_threshold
                    
                    if is_relevant and meets_threshold:
                        passed_articles.append(article)
                    else:
                        filtered_articles.append(article)
                else:
                    filtered_articles.append(article)
        else:
            # ê°œë³„ ë¶„ì„ (ê¸°ì¡´ ë°©ì‹)
            for i, article in enumerate(articles):
                logger.info(f"ë¶„ì„ ì¤‘ ({i+1}/{len(articles)}): {article.title[:50]}...")
                
                filter_result = self.gemini.filter_news(
                    title=article.title,
                    description=article.description,
                    category=article.category
                )
                
                article.relevance_score = filter_result.get("relevance_score", 0)
                article.importance_score = filter_result.get("importance_score", 1)
                
                if filter_result.get("category"):
                    article.category = filter_result["category"]
                
                is_relevant = filter_result.get("is_relevant", False)
                meets_threshold = article.relevance_score >= self.relevance_threshold
                
                if is_relevant and meets_threshold:
                    passed_articles.append(article)
                else:
                    filtered_articles.append(article)
        
        # ê²½ë‚¨ ì§€ì—­ ì–¸ë¡ ì‚¬ ê°€ì‚°ì  ì ìš©
        priority_count = 0
        for article in passed_articles:
            bonus = self._get_media_bonus(article.url)
            if bonus > 0:
                article.importance_score = min(5, (article.importance_score or 1) + bonus)
                priority_count += 1
        
        if priority_count > 0:
            logger.info(f"ğŸ† ê²½ë‚¨ ì§€ì—­ ì–¸ë¡ ì‚¬ ê°€ì‚°ì  ì ìš©: {priority_count}ê±´")
        
        # í†µê³¼ëœ ê¸°ì‚¬ë§Œ ìš”ì•½ ìƒì„±
        if summarize and passed_articles:
            logger.info(f"ìš”ì•½ ìƒì„± ì¤‘: {len(passed_articles)}ê±´...")
            for i, article in enumerate(passed_articles):
                logger.info(f"ìš”ì•½ ì¤‘ ({i+1}/{len(passed_articles)}): {article.title[:40]}...")
                summary_result = self.gemini.summarize_news(
                    title=article.title,
                    content=article.description or article.content,
                    category=article.category
                )
                
                article.one_line_summary = summary_result.get("one_line_summary")
                article.detailed_summary = summary_result.get("detailed_summary")
                article.keywords = summary_result.get("keywords", [])
        
        logger.info(
            f"=== ë¶„ì„ ì™„ë£Œ: í†µê³¼ {len(passed_articles)}ê±´, "
            f"ì œì™¸ {len(filtered_articles)}ê±´ ==="
        )
        
        return passed_articles, filtered_articles
    
    def sort_by_importance(
        self,
        articles: List[NewsArticle],
        reverse: bool = True
    ) -> List[NewsArticle]:
        """ì¤‘ìš”ë„ìˆœ ì •ë ¬
        
        Args:
            articles: ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
            reverse: ë‚´ë¦¼ì°¨ìˆœ ì—¬ë¶€ (Trueë©´ ë†’ì€ ìˆœ)
            
        Returns:
            ì •ë ¬ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        """
        return sorted(
            articles,
            key=lambda x: (x.importance_score or 0, x.relevance_score or 0),
            reverse=reverse
        )
    
    def group_by_category(
        self,
        articles: List[NewsArticle]
    ) -> dict:
        """ì¹´í…Œê³ ë¦¬ë³„ ê·¸ë£¹í™”
        
        Args:
            articles: ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì¹´í…Œê³ ë¦¬ë³„ ê¸°ì‚¬ ë”•ì…”ë„ˆë¦¬
        """
        grouped = {}
        
        for article in articles:
            category = article.category or "ì¼ë°˜"
            if category not in grouped:
                grouped[category] = []
            grouped[category].append(article)
        
        return grouped
    
    def group_by_importance(
        self,
        articles: List[NewsArticle]
    ) -> dict:
        """ì¤‘ìš”ë„ë³„ ê·¸ë£¹í™”
        
        Args:
            articles: ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì¤‘ìš”ë„ë³„ ê¸°ì‚¬ ë”•ì…”ë„ˆë¦¬
        """
        grouped = {
            "urgent": [],      # 5ì 
            "important": [],   # 4ì 
            "normal": [],      # 3ì 
            "low": []          # 1-2ì 
        }
        
        for article in articles:
            score = article.importance_score or 1
            
            if score >= 5:
                grouped["urgent"].append(article)
            elif score >= 4:
                grouped["important"].append(article)
            elif score >= 3:
                grouped["normal"].append(article)
            else:
                grouped["low"].append(article)
        
        return grouped
    
    def _normalize_title(self, title: str) -> str:
        """ì œëª© ì •ê·œí™” (ë¹„êµë¥¼ ìœ„í•´)"""
        # ì–¸ë¡ ì‚¬ëª… ì œê±° (- ë’¤ì˜ ë‚´ìš©)
        title = re.sub(r'\s*[-â€“â€”]\s*[^-â€“â€”]+$', '', title)
        # íŠ¹ìˆ˜ë¬¸ì ì œê±°
        title = re.sub(r'[^\w\sê°€-í£]', '', title)
        # ê³µë°± ì •ê·œí™”
        title = ' '.join(title.split())
        return title.strip()
    
    def _calculate_similarity(self, title1: str, title2: str) -> float:
        """ë‘ ì œëª©ì˜ ìœ ì‚¬ë„ ê³„ì‚° (0~1)"""
        norm1 = self._normalize_title(title1)
        norm2 = self._normalize_title(title2)
        return SequenceMatcher(None, norm1, norm2).ratio()
    
    def deduplicate_similar_news(
        self,
        articles: List[NewsArticle],
        similarity_threshold: float = 0.6
    ) -> List[NewsArticle]:
        """ìœ ì‚¬í•œ ë‰´ìŠ¤ë¥¼ ê·¸ë£¹í™”í•˜ì—¬ ì¤‘ë³µ ì œê±°
        
        ë¹„ìŠ·í•œ ë‰´ìŠ¤ëŠ” ëŒ€í‘œ ë‰´ìŠ¤ í•˜ë‚˜ì— ê´€ë ¨ ë§í¬ë¡œ ë¬¶ìŒ
        
        Args:
            articles: ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
            similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’ (0.6 = 60% ì´ìƒ ìœ ì‚¬í•˜ë©´ ê°™ì€ ë‰´ìŠ¤ë¡œ íŒë‹¨)
            
        Returns:
            ì¤‘ë³µ ì œê±°ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ (ê° ê¸°ì‚¬ì— related_urls ì†ì„± ì¶”ê°€)
        """
        if not articles:
            return []
        
        # ì´ë¯¸ ê·¸ë£¹ì— í¬í•¨ëœ ê¸°ì‚¬ ì¸ë±ìŠ¤
        grouped_indices = set()
        result = []
        
        for i, article in enumerate(articles):
            if i in grouped_indices:
                continue
            
            # ì´ ê¸°ì‚¬ì™€ ìœ ì‚¬í•œ ê¸°ì‚¬ë“¤ ì°¾ê¸°
            similar_articles = []
            
            for j, other in enumerate(articles):
                if i == j or j in grouped_indices:
                    continue
                
                similarity = self._calculate_similarity(article.title, other.title)
                
                if similarity >= similarity_threshold:
                    similar_articles.append(other)
                    grouped_indices.add(j)
            
            # ëŒ€í‘œ ê¸°ì‚¬ ì„ ì • (ìš°ì„ ìˆœìœ„: ê²½ë‚¨ ì§€ì—­ ì–¸ë¡ ì‚¬ > ì¤‘ìš”ë„ > ê´€ë ¨ì„± > ë‚´ìš© ê¸¸ì´)
            all_similar = [article] + similar_articles
            representative = max(
                all_similar,
                key=lambda x: (
                    self._is_priority_media(x.url),  # ê²½ë‚¨ ì§€ì—­ ì–¸ë¡ ì‚¬ ìš°ì„ 
                    x.importance_score or 0,
                    x.relevance_score or 0,
                    len(x.description or '')
                )
            )
            
            # ê´€ë ¨ URL ëª©ë¡ ìƒì„± (ëŒ€í‘œ ê¸°ì‚¬ ì œì™¸)
            related_urls = []
            for similar in all_similar:
                if similar.url != representative.url:
                    related_urls.append({
                        'title': similar.title,
                        'url': similar.url,
                        'media': similar.media_name or 'ì•Œ ìˆ˜ ì—†ìŒ'
                    })
            
            # ëŒ€í‘œ ê¸°ì‚¬ì— ê´€ë ¨ URL ì¶”ê°€
            representative.related_urls = related_urls
            
            result.append(representative)
            grouped_indices.add(i)
        
        logger.info(f"ì¤‘ë³µ ì œê±°: {len(articles)}ê±´ â†’ {len(result)}ê±´ ({len(articles) - len(result)}ê±´ ê·¸ë£¹í™”)")
        
        return result

